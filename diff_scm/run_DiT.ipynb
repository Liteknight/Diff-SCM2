{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "355UKMUQJxFd"
   },
   "source": [
    "# Scalable Diffusion Models with Transformer (DiT)\n",
    "\n",
    "This notebook samples from pre-trained DiT models. DiTs are class-conditional latent diffusion models trained on ImageNet that use transformers in place of U-Nets as the DDPM backbone. DiT outperforms all prior diffusion models on the ImageNet benchmarks.\n",
    "\n",
    "[Project Page](https://www.wpeebles.com/DiT) | [HuggingFace Space](https://huggingface.co/spaces/wpeebles/DiT) | [Paper](http://arxiv.org/abs/2212.09748) | [GitHub](github.com/facebookresearch/DiT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJlgLkSaKn7u"
   },
   "source": [
    "# 1. Setup\n",
    "\n",
    "We recommend using GPUs (Runtime > Change runtime type > Hardware accelerator > GPU). Run this cell to clone the DiT GitHub repo and setup PyTorch. You only have to run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T22:25:36.739559777Z",
     "start_time": "2023-05-23T22:25:28.292339172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DiT'...\r\n",
      "remote: Enumerating objects: 99, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (74/74), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (38/38), done.\u001B[K\r\n",
      "remote: Total 99 (delta 52), reused 36 (delta 36), pack-reused 25\u001B[K\r\n",
      "Receiving objects: 100% (99/99), 6.36 MiB | 1.51 MiB/s, done.\r\n",
      "Resolving deltas: 100% (54/54), done.\r\n",
      "Requirement already satisfied: diffusers in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (0.16.1)\r\n",
      "Requirement already satisfied: timm in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (0.9.2)\r\n",
      "Requirement already satisfied: Pillow in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from diffusers) (9.4.0)\r\n",
      "Requirement already satisfied: filelock in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from diffusers) (3.9.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from diffusers) (0.14.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from diffusers) (6.6.0)\r\n",
      "Requirement already satisfied: numpy in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from diffusers) (1.24.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from diffusers) (2023.5.5)\r\n",
      "Requirement already satisfied: requests in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from diffusers) (2.29.0)\r\n",
      "Requirement already satisfied: torch>=1.7 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from timm) (2.0.1)\r\n",
      "Requirement already satisfied: torchvision in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from timm) (0.15.2)\r\n",
      "Requirement already satisfied: pyyaml in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from timm) (6.0)\r\n",
      "Requirement already satisfied: safetensors in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from timm) (0.3.1)\r\n",
      "Requirement already satisfied: fsspec in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers) (2023.5.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers) (4.65.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers) (4.5.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers) (23.1)\r\n",
      "Requirement already satisfied: sympy in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from torch>=1.7->timm) (1.11.1)\r\n",
      "Requirement already satisfied: networkx in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from torch>=1.7->timm) (2.8.4)\r\n",
      "Requirement already satisfied: jinja2 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from torch>=1.7->timm) (3.1.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from importlib-metadata->diffusers) (3.15.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from requests->diffusers) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from requests->diffusers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from requests->diffusers) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from requests->diffusers) (2023.5.7)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from jinja2->torch>=1.7->timm) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/fvamosi/anaconda3/envs/DiT/lib/python3.11/site-packages (from sympy->torch>=1.7->timm) (1.2.1)\r\n",
      "Using GPU.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/DiT.git\n",
    "import DiT, os\n",
    "os.chdir('DiT')\n",
    "os.environ['PYTHONPATH'] = '/env/python:/content/DiT'\n",
    "!pip install diffusers timm --upgrade\n",
    "# DiT imports:\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from diffusion import create_diffusion\n",
    "from diffusers.models import AutoencoderKL\n",
    "from download import find_model\n",
    "from models import DiT_XL_2\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    print(\"GPU not found. Using CPU instead.\")\n",
    "else:\n",
    "    print(\"Using GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXpziRkoOvV9"
   },
   "source": [
    "# Download DiT-XL/2 Models\n",
    "\n",
    "You can choose between a 512x512 model and a 256x256 model. You can swap-out the LDM VAE, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EWG-WNimO59K",
    "ExecuteTime": {
     "end_time": "2023-05-23T22:23:14.491409309Z",
     "start_time": "2023-05-23T22:18:08.586862602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://dl.fbaipublicfiles.com/DiT/models/DiT-XL-2-256x256.pt to pretrained_models/DiT-XL-2-256x256.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2700611775/2700611775 [04:52<00:00, 9242633.54it/s]\n"
     ]
    }
   ],
   "source": [
    "image_size = 256 #@param [256, 512]\n",
    "vae_model = \"stabilityai/sd-vae-ft-ema\" #@param [\"stabilityai/sd-vae-ft-mse\", \"stabilityai/sd-vae-ft-ema\"]\n",
    "latent_size = int(image_size) // 8\n",
    "# Load model:\n",
    "model = DiT_XL_2(input_size=latent_size).to(device)\n",
    "state_dict = find_model(f\"DiT-XL-2-{image_size}x{image_size}.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval() # important!\n",
    "vae = AutoencoderKL.from_pretrained(vae_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JTNyzNZKb9E"
   },
   "source": [
    "# 2. Sample from Pre-trained DiT Models\n",
    "\n",
    "You can customize several sampling options. For the full list of ImageNet classes, [check out this](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-Hw7B5h4Kk4p",
    "ExecuteTime": {
     "end_time": "2023-05-23T22:26:27.417063660Z",
     "start_time": "2023-05-23T22:26:27.224852477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f8fc6706c374fa28e7d989342b79f59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 3.95 GiB total capacity; 3.14 GiB already allocated; 17.69 MiB free; 3.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 26\u001B[0m\n\u001B[1;32m     23\u001B[0m model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(y\u001B[38;5;241m=\u001B[39my, cfg_scale\u001B[38;5;241m=\u001B[39mcfg_scale)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Sample images:\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m samples \u001B[38;5;241m=\u001B[39m diffusion\u001B[38;5;241m.\u001B[39mp_sample_loop(\n\u001B[1;32m     27\u001B[0m     model\u001B[38;5;241m.\u001B[39mforward_with_cfg, z\u001B[38;5;241m.\u001B[39mshape, z, clip_denoised\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, \n\u001B[1;32m     28\u001B[0m     model_kwargs\u001B[38;5;241m=\u001B[39mmodel_kwargs, progress\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, device\u001B[38;5;241m=\u001B[39mdevice\n\u001B[1;32m     29\u001B[0m )\n\u001B[1;32m     30\u001B[0m samples, _ \u001B[38;5;241m=\u001B[39m samples\u001B[38;5;241m.\u001B[39mchunk(\u001B[38;5;241m2\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# Remove null class samples\u001B[39;00m\n\u001B[1;32m     31\u001B[0m samples \u001B[38;5;241m=\u001B[39m vae\u001B[38;5;241m.\u001B[39mdecode(samples \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m0.18215\u001B[39m)\u001B[38;5;241m.\u001B[39msample\n",
      "File \u001B[0;32m/media/data/finn/PycharmProjects/DiT/diffusion/gaussian_diffusion.py:450\u001B[0m, in \u001B[0;36mGaussianDiffusion.p_sample_loop\u001B[0;34m(self, model, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001B[0m\n\u001B[1;32m    431\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;124;03mGenerate samples from the model.\u001B[39;00m\n\u001B[1;32m    433\u001B[0m \u001B[38;5;124;03m:param model: the model module.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    447\u001B[0m \u001B[38;5;124;03m:return: a non-differentiable batch of samples.\u001B[39;00m\n\u001B[1;32m    448\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    449\u001B[0m final \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 450\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp_sample_loop_progressive(\n\u001B[1;32m    451\u001B[0m     model,\n\u001B[1;32m    452\u001B[0m     shape,\n\u001B[1;32m    453\u001B[0m     noise\u001B[38;5;241m=\u001B[39mnoise,\n\u001B[1;32m    454\u001B[0m     clip_denoised\u001B[38;5;241m=\u001B[39mclip_denoised,\n\u001B[1;32m    455\u001B[0m     denoised_fn\u001B[38;5;241m=\u001B[39mdenoised_fn,\n\u001B[1;32m    456\u001B[0m     cond_fn\u001B[38;5;241m=\u001B[39mcond_fn,\n\u001B[1;32m    457\u001B[0m     model_kwargs\u001B[38;5;241m=\u001B[39mmodel_kwargs,\n\u001B[1;32m    458\u001B[0m     device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[1;32m    459\u001B[0m     progress\u001B[38;5;241m=\u001B[39mprogress,\n\u001B[1;32m    460\u001B[0m ):\n\u001B[1;32m    461\u001B[0m     final \u001B[38;5;241m=\u001B[39m sample\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m final[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msample\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/media/data/finn/PycharmProjects/DiT/diffusion/gaussian_diffusion.py:501\u001B[0m, in \u001B[0;36mGaussianDiffusion.p_sample_loop_progressive\u001B[0;34m(self, model, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001B[0m\n\u001B[1;32m    499\u001B[0m t \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mtensor([i] \u001B[38;5;241m*\u001B[39m shape[\u001B[38;5;241m0\u001B[39m], device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 501\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp_sample(\n\u001B[1;32m    502\u001B[0m         model,\n\u001B[1;32m    503\u001B[0m         img,\n\u001B[1;32m    504\u001B[0m         t,\n\u001B[1;32m    505\u001B[0m         clip_denoised\u001B[38;5;241m=\u001B[39mclip_denoised,\n\u001B[1;32m    506\u001B[0m         denoised_fn\u001B[38;5;241m=\u001B[39mdenoised_fn,\n\u001B[1;32m    507\u001B[0m         cond_fn\u001B[38;5;241m=\u001B[39mcond_fn,\n\u001B[1;32m    508\u001B[0m         model_kwargs\u001B[38;5;241m=\u001B[39mmodel_kwargs,\n\u001B[1;32m    509\u001B[0m     )\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m out\n\u001B[1;32m    511\u001B[0m     img \u001B[38;5;241m=\u001B[39m out[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msample\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/media/data/finn/PycharmProjects/DiT/diffusion/gaussian_diffusion.py:402\u001B[0m, in \u001B[0;36mGaussianDiffusion.p_sample\u001B[0;34m(self, model, x, t, clip_denoised, denoised_fn, cond_fn, model_kwargs)\u001B[0m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mp_sample\u001B[39m(\n\u001B[1;32m    377\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    378\u001B[0m     model,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    384\u001B[0m     model_kwargs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    385\u001B[0m ):\n\u001B[1;32m    386\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;124;03m    Sample x_{t-1} from the model at the given timestep.\u001B[39;00m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;124;03m    :param model: the model to sample from.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    400\u001B[0m \u001B[38;5;124;03m             - 'pred_xstart': a prediction of x_0.\u001B[39;00m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 402\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp_mean_variance(\n\u001B[1;32m    403\u001B[0m         model,\n\u001B[1;32m    404\u001B[0m         x,\n\u001B[1;32m    405\u001B[0m         t,\n\u001B[1;32m    406\u001B[0m         clip_denoised\u001B[38;5;241m=\u001B[39mclip_denoised,\n\u001B[1;32m    407\u001B[0m         denoised_fn\u001B[38;5;241m=\u001B[39mdenoised_fn,\n\u001B[1;32m    408\u001B[0m         model_kwargs\u001B[38;5;241m=\u001B[39mmodel_kwargs,\n\u001B[1;32m    409\u001B[0m     )\n\u001B[1;32m    410\u001B[0m     noise \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mrandn_like(x)\n\u001B[1;32m    411\u001B[0m     nonzero_mask \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    412\u001B[0m         (t \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39m([\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m*\u001B[39m (\u001B[38;5;28mlen\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)))\n\u001B[1;32m    413\u001B[0m     )  \u001B[38;5;66;03m# no noise when t == 0\u001B[39;00m\n",
      "File \u001B[0;32m/media/data/finn/PycharmProjects/DiT/diffusion/respace.py:92\u001B[0m, in \u001B[0;36mSpacedDiffusion.p_mean_variance\u001B[0;34m(self, model, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mp_mean_variance\u001B[39m(\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;28mself\u001B[39m, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m     91\u001B[0m ):  \u001B[38;5;66;03m# pylint: disable=signature-differs\u001B[39;00m\n\u001B[0;32m---> 92\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mp_mean_variance(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_model(model), \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/media/data/finn/PycharmProjects/DiT/diffusion/gaussian_diffusion.py:279\u001B[0m, in \u001B[0;36mGaussianDiffusion.p_mean_variance\u001B[0;34m(self, model, x, t, clip_denoised, denoised_fn, model_kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m B, C \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m t\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m (B,)\n\u001B[0;32m--> 279\u001B[0m model_output \u001B[38;5;241m=\u001B[39m model(x, t, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model_output, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    281\u001B[0m     model_output, extra \u001B[38;5;241m=\u001B[39m model_output\n",
      "File \u001B[0;32m/media/data/finn/PycharmProjects/DiT/diffusion/respace.py:129\u001B[0m, in \u001B[0;36m_WrappedModel.__call__\u001B[0;34m(self, x, ts, **kwargs)\u001B[0m\n\u001B[1;32m    126\u001B[0m new_ts \u001B[38;5;241m=\u001B[39m map_tensor[ts]\n\u001B[1;32m    127\u001B[0m \u001B[38;5;66;03m# if self.rescale_timesteps:\u001B[39;00m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;66;03m#     new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\u001B[39;00m\n\u001B[0;32m--> 129\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(x, new_ts, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/media/data/finn/PycharmProjects/DiT/models.py:257\u001B[0m, in \u001B[0;36mDiT.forward_with_cfg\u001B[0;34m(self, x, t, y, cfg_scale)\u001B[0m\n\u001B[1;32m    255\u001B[0m half \u001B[38;5;241m=\u001B[39m x[: \u001B[38;5;28mlen\u001B[39m(x) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    256\u001B[0m combined \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([half, half], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m--> 257\u001B[0m model_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(combined, t, y)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;66;03m# For exact reproducibility reasons, we apply classifier-free guidance on only\u001B[39;00m\n\u001B[1;32m    259\u001B[0m \u001B[38;5;66;03m# three channels by default. The standard approach to cfg applies it to all channels.\u001B[39;00m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;66;03m# This can be done by uncommenting the following line and commenting-out the line following that.\u001B[39;00m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;66;03m# eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]\u001B[39;00m\n\u001B[1;32m    262\u001B[0m eps, rest \u001B[38;5;241m=\u001B[39m model_out[:, :\u001B[38;5;241m3\u001B[39m], model_out[:, \u001B[38;5;241m3\u001B[39m:]\n",
      "File \u001B[0;32m/media/data/finn/PycharmProjects/DiT/models.py:245\u001B[0m, in \u001B[0;36mDiT.forward\u001B[0;34m(self, x, t, y)\u001B[0m\n\u001B[1;32m    243\u001B[0m c \u001B[38;5;241m=\u001B[39m t \u001B[38;5;241m+\u001B[39m y                                \u001B[38;5;66;03m# (N, D)\u001B[39;00m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks:\n\u001B[0;32m--> 245\u001B[0m     x \u001B[38;5;241m=\u001B[39m block(x, c)                      \u001B[38;5;66;03m# (N, T, D)\u001B[39;00m\n\u001B[1;32m    246\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinal_layer(x, c)                \u001B[38;5;66;03m# (N, T, patch_size ** 2 * out_channels)\u001B[39;00m\n\u001B[1;32m    247\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munpatchify(x)                   \u001B[38;5;66;03m# (N, out_channels, H, W)\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/DiT/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/media/data/finn/PycharmProjects/DiT/models.py:120\u001B[0m, in \u001B[0;36mDiTBlock.forward\u001B[0;34m(self, x, c)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, c):\n\u001B[1;32m    119\u001B[0m     shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madaLN_modulation(c)\u001B[38;5;241m.\u001B[39mchunk(\u001B[38;5;241m6\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 120\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m gate_msa\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn(modulate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x), shift_msa, scale_msa))\n\u001B[1;32m    121\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m gate_mlp\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(modulate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x), shift_mlp, scale_mlp))\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m/media/data/finn/PycharmProjects/DiT/models.py:20\u001B[0m, in \u001B[0;36mmodulate\u001B[0;34m(x, shift, scale)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmodulate\u001B[39m(x, shift, scale):\n\u001B[0;32m---> 20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m scale\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)) \u001B[38;5;241m+\u001B[39m shift\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 3.95 GiB total capacity; 3.14 GiB already allocated; 17.69 MiB free; 3.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Set user inputs:\n",
    "seed = 0 #@param {type:\"number\"}\n",
    "torch.manual_seed(seed)\n",
    "num_sampling_steps = 1 #250 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "cfg_scale = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "class_labels = 207, 360, 387, 974, 88, 979, 417, 279 #@param {type:\"raw\"}\n",
    "samples_per_row = 4 #@param {type:\"number\"}\n",
    "\n",
    "max_split_size_mb=1024\n",
    "\n",
    "# Create diffusion object:\n",
    "diffusion = create_diffusion(str(num_sampling_steps))\n",
    "\n",
    "# Create sampling noise:\n",
    "n = len(class_labels)\n",
    "z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
    "y = torch.tensor(class_labels, device=device)\n",
    "\n",
    "# Setup classifier-free guidance:\n",
    "z = torch.cat([z, z], 0)\n",
    "y_null = torch.tensor([1000] * n, device=device)\n",
    "y = torch.cat([y, y_null], 0)\n",
    "model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "\n",
    "# Sample images:\n",
    "samples = diffusion.p_sample_loop(\n",
    "    model.forward_with_cfg, z.shape, z, clip_denoised=False, \n",
    "    model_kwargs=model_kwargs, progress=True, device=device\n",
    ")\n",
    "samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "samples = vae.decode(samples / 0.18215).sample\n",
    "\n",
    "# Save and display images:\n",
    "save_image(samples, \"sample.png\", nrow=int(samples_per_row), \n",
    "           normalize=True, value_range=(-1, 1))\n",
    "samples = Image.open(\"sample.png\")\n",
    "display(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
